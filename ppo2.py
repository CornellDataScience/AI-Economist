# from https://github.com/dickreuter/neuron_poker

# form https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8
# it seems that PPO is actor loss, total loss = actor + critic * discount - entropy