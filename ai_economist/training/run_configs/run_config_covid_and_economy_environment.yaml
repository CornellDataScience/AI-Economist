# Copyright (c) 2021, salesforce.com, inc.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root
# or https://opensource.org/licenses/BSD-3-Clause
#
# YAML configuration for the tag continuous environment
name: "covid_and_economy_environment"
# Environment settings
env:
    collate_agent_step_and_reset_data: True
    components:
        - ControlUSStateOpenCloseStatus: 
            action_cooldown_period: 28
        - FederalGovernmentSubsidy:
            num_subsidy_levels: 20
            subsidy_interval: 90
            max_annual_subsidy_per_person: 20000
        - VaccinationCampaign:
            daily_vaccines_per_million_people: 3000
            delivery_interval: 1
            vaccine_delivery_start_date: "2021-01-12"
    economic_reward_crra_eta: 2
    episode_length: 540
    flatten_masks: True
    flatten_observations: False
    health_priority_scaling_agents: 0.3
    health_priority_scaling_planner: 0.45
    infection_too_sick_to_work_rate: 0.1
    multi_action_mode_agents: False
    multi_action_mode_planner: False
    n_agents: 51
    path_to_data_and_fitted_params: ""
    pop_between_age_18_65: 0.6
    risk_free_interest_rate: 0.03
    world_size: [1, 1]
    start_date: "2020-03-22"
    use_real_world_data: False
    use_real_world_policies: False
# Trainer settings
trainer:
    num_envs: 60 # Number of environment replicas
    num_episodes: 1000000000 # Number of episodes to run the training for
    train_batch_size: 5400 # total batch size used for training per iteration (across all the environments)
    algorithm: "A2C" # trainer algorithm
    vf_loss_coeff: 1 # loss coefficient for the value function loss
    entropy_coeff: 0.05 # coefficient for the entropy component of the loss
    clip_grad_norm: True # fla indicating whether to clip the gradient norm or not
    max_grad_norm: 0.5 # when clip_grad_norm is True, the clip level
    normalize_advantage: False # flag indicating whether to normalize advantage or not
    normalize_return: False # flag indicating whether to normalize return or not
# Policy network settings
policy: # list all the policies below
    a:
        to_train: True
        name: "fully_connected"
        gamma: 0.98 # discount rate gamms
        lr: 0.005 # learning rate
        model:
        # dimension(s) of the fully connected layers as a list
            fc_dims: [256, 256]
            model_ckpt_filepath: ""
    p:
        to_train: True
        name: "fully_connected"
        gamma: 0.98
        lr: 0.002
        model:
            fc_dims: [256, 256]
            model_ckpt_filepath: ""
# Checkpoint saving setting (and W&B logging)
saving:
    print_metrics_freq: 100 # How often (in iterations) to print the metrics
    save_model_params_freq: 5000 # How often (in iterations) to save the model parameters
    basedir: "/tmp" # base folder used for saving
    tag: "experiments"
