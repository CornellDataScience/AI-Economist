
# YAML configuration for the two-layer AI Economist experiments.
name: "simple_wood_and_stone_environment"

# Environment settings
env:
    checker_source_blocks: False
    collate_agent_step_and_reset_data: False
    components:
        - Build:
            payment: 10
            payment_max_skill_multiplier: 1
            skill_dist: "none"
            build_labor: 10.0
        - ContinuousDoubleAuction:
            max_bid_ask: 10
            order_labor: 0.25
            order_duration: 50
        - Gather:
            move_labor: 1.0
            collect_labor: 1.0
            skill_dist: "none"
        - WealthRedistribution:
    economic_reward_crra_eta: 2
    energy_cost: 0.21
    episode_length: 1000
    flatten_masks: True
    flatten_observations: False
    full_observability: False
    gradient_steepness: 0.4
    isoelastic_eta: 0.23
    mobile_agent_observation_range: 11
    multi_action_mode_agents: False
    multi_action_mode_planner: False
    n_agents: 5
    path_to_data_and_fitted_params: ""
    planner_gets_spatial_obs: False
    risk_free_interest_rate: 0.03
    starting_agent_coin: 0
    starting_stone_coverage: 20
    starting_wood_coverage: 20
    stone_clumpiness: 0.5
    stone_max_health: 1
    stone_regen_halfwidth: 2
    stone_regen_weight: 0.01
    wood_clumpiness: 0.5
    wood_max_health: 1
    wood_regen_halfwidth: 2
    wood_regen_weight: 0.01
    world_size: [25, 25]
# Trainer settings
trainer:
    num_envs: 60 # Number of environment replicas
    num_episodes: 1000000000 # Number of episodes to run the training for
    train_batch_size: 5400 # total batch size used for training per iteration (across all the environments)
    algorithm: "A2C" # trainer algorithm
    vf_loss_coeff: 1 # loss coefficient for the value function loss
    entropy_coeff: 0.05 # coefficient for the entropy component of the loss
    clip_grad_norm: True # fla indicating whether to clip the gradient norm or not
    max_grad_norm: 0.5 # when clip_grad_norm is True, the clip level
    normalize_advantage: False # flag indicating whether to normalize advantage or not
    normalize_return: False # flag indicating whether to normalize return or not
# Policy network settings
policy: # list all the policies below
    a:
        to_train: True
        name: "fully_connected"
        gamma: 0.98 # discount rate gamms
        lr: 0.005 # learning rate
        model:
        # dimension(s) of the fully connected layers as a list
            fc_dims: [256, 256]
            model_ckpt_filepath: ""
    p:
        to_train: True
        name: "fully_connected"
        gamma: 0.98
        lr: 0.002
        model:
            fc_dims: [256, 256]
            model_ckpt_filepath: ""
# Checkpoint saving setting (and W&B logging)
saving:
    print_metrics_freq: 100 # How often (in iterations) to print the metrics
    save_model_params_freq: 5000 # How often (in iterations) to save the model parameters
    basedir: "/tmp" # base folder used for saving
    tag: "experiments"